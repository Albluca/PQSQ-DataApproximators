%% This is file `elsarticle-template-1-num.tex',
%%
%% Copyright 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%%
%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%%
%% $Id: elsarticle-template-1-num.tex 149 2009-10-08 05:01:15Z rishi $
%% $URL: http://lenova.river-valley.com/svn/elsbst/trunk/elsarticle-template-1-num.tex $
%%
\documentclass[preprint,12pt,twocolumn]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% if you use PostScript figures in your article
%% use the graphics package for simple commands
%% \usepackage{graphics}
%% or use the graphicx package for more complicated commands
%% \usepackage{graphicx}
%% or use the epsfig package if you prefer to use the old commands
%% \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{xcolor}
\newcommand\myworries[1]{\textcolor{red}{#1}}
%\usepackage{hyperref}


%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
\usepackage{lineno}

\newtheorem{prop}{Proposition}
\newtheorem{theorem}{Theorem}[section]
\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\DeclareMathOperator*{\argmin}{arg\,min}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}


%\journal{Journal Name}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\begin{document}

\twocolumn[{\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{Piece-wise quadratic lego set for constructing data approximation potentials and their fast optimization}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\author{Alexander N. Gorban$^{1}$, Evgeny M. Mirkes$^{1}$, Andrei Zinovyev$^2$}

\address{$^{1}$ Department of Mathematics, University of Leicester, University Road, Leicester LE1 7RH, UK \\ $^{2}$   Institut Curie, PSL Research University, Mines Paris Tech, Inserm, U900, F-75005, Paris, France. }

\begin{abstract}
Data dimension reduction by constructing low-dimensional approximators is one of the most fundamental approaches in data mining.
Most efficient approximators based on quadratic error functional are not flexible enough in many circumstances and suffer from sensitivity to outliers
and dimensionality curse, which led to introducing other functional forms of error potential that are usually computationally expensive to minimize.
We suggest using piece-wise quadratic error potentials of subquadratic growth (PQSQ potentials) that can imitate a variety
of approximation metrics used in practice, such as the popular $L1$-norm or even sub-linear potentials corresponding to fractional norm.
A family of subquadratic piece-wise potentials are almost as computationally
efficient in numerical optimization as quadratic ones for the most popular data approximators ($k$-means, principal components, principal manifolds and graphs),
has guaranteed convergence to the global or local error minimum, allows flexible choice of data approximation metrics and can be naturally
robust to outlier data points. We introduce this family of potentials, provide implementations of several popular data approximators
exploiting them and do benchmarking.
\end{abstract}

\begin{keyword}
data approximation \sep nonquadraic potential \sep principal components \sep clustering
%% keywords here, in the form: keyword \sep keyword

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

}]

%%
%% Start line numbering here if you want
%%
%\linenumbers

%% main text
\section{Introduction}
\label{S:1}

Data dimension reduction by constructing explicit low-dimensional approximators of a finite set of vectors is one of the most fundamental approach in data analysis. Starting from the classical data approximators such as $k$-means \cite{Lloyd1957} and linear principal components (PCA) \cite{Pearson1901On}, multiple generalizations have been suggested in the last decades (self-organizing maps, principal curves, principal manifolds, principal graphs, principal trees, etc.)\cite{Gorban2009,Gorban2008Principal} in order to make the data approximators more flexible and suitable for complex data structures.

We solve the problem of approximating a finite set of vectors ${\vec{x}_i}\in R^m,i=1...N$ (data set) by a simpler object $L$ embedded into the data space, such that for each point $\vec{x}_i$ an approximation error $err(\vec{x}_i,L)$ function can be defined. We assume this function in the form

\begin{equation}\label{distance_function}
err(\vec{x}_i,L) = \min_{y\in L} \sum_k u(x_i^k-y^k),
\end{equation}

\noindent where the upper $k=1...m$ stands for the coordinate index, and $u(x)$ is a monotonously growing symmetrical function, which we will be calling the error potential. By data approximation we mean that the embedment of $L$ in the data space minimizes the error

$$
\sum_i err(\vec{x}_i,L) \rightarrow \min.
$$

Note that our definition of error function is coordinate-wise (it is a sum of error potential over all coordinates).

The simplest form of the error potential is quadratic $u(x)=x^2$, which leads to the most known data approximators: mean point ($L$ is a point), principal points ($L$ is a set of points) \cite{Flury1990}, principal components ($L$ is a line or a hyperplane) \cite{Pearson1901On}. In more advanced cases, $L$ can posses some regular properties leading to principal curves ($L$ is a smooth line or spline) \cite{Hastie1984}, principal manifolds ($L$ is a smooth low-dimensional surface) and principal graphs (eg., $L$ is a pluri-harmonic graph embedment) \cite{gorban2007topological,Gorban2009}.

There exist multiple advantages of using quadratic potential $u(x)$, because it leads to the most computationally efficient algorithms usually based on the splitting schema, a variant of Expectation-Minimization approach \cite{Gorban2009}. For example, $k$-means algorithm solves the problem of finding the set of principal points and the standard iterative Singular Value Decomposition finds principal components. However, quadratic potential is known to be sensitive to outliers in the data set. Also, purely quadratic potentials can suffer from the curse of dimensionality, not being able to robustly discriminate ``close" and ``distant" point neighbours in a high-dimensional space \cite{Aggarwal2001}.

There exist several widely used ideas for increasing approximator's robustness in the presence of strong noise in data such as: (1) using medians instead of mean values, (2) substituting quadratic norm by L1 norm (e.g. \cite{Ding2006, hauberg2014}), (3) outliers exclusion or fixed weighting or iterative reweighting during optimizing the data approximators (e.g. \cite{Xu1995,Allende2004,kohonen2001self}), and (4) regularizing the PCA vectors by L1 norm \cite{Jolliffe2003,Candes2011,Zou2006}. In some works, it was suggested to utilize ``trimming'' averages, e.g. in the context of the $k$-means clustering or some generalizations of PCA \cite{cuesta1997,hauberg2014}). In the context of regression, iterative reweighting is exploited to mimic the properties of L1 norm \cite{Lu2015}. Several algorithms for constructing PCA with L1 norm have been suggested \cite{Ke2005,Kwak2008,Brooks2013} and systematically benchmarked \cite{brooks2012pcal1,Park2014}. Some authors go even beyond linear metrics and suggests that fractional norms (L$p$ metrics with $p<1$) can be more appropriate in high-dimensional data approximation \cite{Aggarwal2001}.

However, most of the suggested approaches exploiting properties of non-quadratic metrics either represent useful but still arbitrary heuristics or are not sufficiently scalable. The standard approach for minimizing L1-based norm consists in solving a linear programming task. Despite existence of many efficient linear programming optimizer implementations, by their nature these computations are much slower than the iterative methods used in the standard SVD algorithm or $k$-means.

In this paper, we exploit the fact that finding a minimum of a piece-wise quadratic function, or, in other words, a function which is the {\it minorant of a set of quadratic functionals}, can be almost as computationally efficient as optimizing the standard quadratic potential. Therefore, if a given arbitrary potential (such as L1-based or even fractional norm-based) can be approximated by a piece-wise quadratic function, this should lead to relatively efficient and simple optimization algorithms. It appears that only potentials of quadratic or subquadratic growth are possible in this approach: however, these are the most usefull ones in data analysis. We introduce a rich family of piece-wise quadratic potentials of subquadratic growth (PQSQ-potentials), suggest general approach for their optimization, prove convergence of a simple iterative algorithm in the most general case, and provide implementations of the standard data approximators (mean point, $k$-means, principal components) using a PQSQ potential. In addition, we discuss exploiting similar approach in non-linear data approximation (i.e. principal manifolds and graphs) and regularized regression (i.e., lasso and elastic nets).


\section{Piecewise quadratic potential of subquadratic growth (PQSQ)}
\label{S:2}

\subsection{Definition of the PQSQ potential}

Let us split all non-negative numbers $x\in R_{\geq 0}$ into $p+1$ non-intersecting intervals $R_0=[0;r_1), R_1=[r_1;r_2), ... , R_k=[r_k;r_{k+1}), ..., R_p=[r_p;\infty)$,  for a set of thresholds $r_1<r_2<...<r_p$. For convenience, let us denote $r_0=0, r_{p+1} = \infty$. Piecewise quadratic potential is a continuous monotonously growing function $u(x)$ constructed from pieces of centered at zero parabolas $y=b_k+a_kx^2$, defined on intervals $x\in[r_k,r_{k+1})$ in the following way (see Figure~\ref{potential}):

\begin{equation}\label{PQSQ_f}
u(x)=
%\begin{cases}
b_k+a_kx^2, \mbox{if } r_k \leq |x|<r_{k+1}, k=0...p,
%\end{cases}
\end{equation}

\begin{equation}\label{PQSQ_acoeffs}
a_k = \frac{f(r_k)-f(r_{k+1})}{r_k^2-r_{k+1}^2},
\end{equation}

\begin{equation}\label{PQSQ_bcoeffs}
b_k = \frac{f(r_{k+1})r_k^2-f(r_{k})r_{k+1}^2}{r_k^2-r_{k+1}^2}
\end{equation}

\noindent where $f(x)$ is a majorating function, which is to be approximated (imitated) by $u(x)$. For example, in the simplest case $f(x)$ can be a linear function: $f(x)=x$, in this case, $\sum_k u(x^k)$ will approximate the $L1$-based error function.

Note that accordingly to (\ref{PQSQ_acoeffs},\ref{PQSQ_bcoeffs}), $b_0=0, a_p=0, b_p=f(r_p)$. Therefore, the choice of $r_p$ can naturally create a ``trimmed'' version of error potential $u(x)$ such that some data points (outliers) do not have any contribution to the gradient of $u(x)$, hence, will not affect the optimization procedure. However, this set of points can change during minimizaton of the potential.

The condition of subquadratic growth consists in the requirement $a_{k+1}\leq a_{k}$ and $b_{k+1} \geq b_{k}$. To guarantee this, the following simple condition on $f(x)$ should be satisfied:

\begin{equation}
\label{eq:condition_function}
f'>0, \>\>\> f''x \leq f',
\end{equation}

\noindent i.e., $f(x)$ should grow not faster than any parabola $ax^2+cx, c>0$.

\begin{figure}[h]
\centering\includegraphics[width=0.9\linewidth]{potential.eps}
\caption{Trimmed piecewise quadratic potential of subquadratic growth $u(x)$ (solid blue line) defined for the majorating function $f(x)$ (red dashed line) and several thresholds $r_k$. Dotted lines show the parabolas which fragments are used to construct $u(x)$. The last parabola is flat ($a_p=0$) which corresponds to trimmed potential. \label{potential}}
\end{figure}

\subsection{Basic approach for optimization}

In order to use the PQSQ potential in an algorithm, two ingredients should be pre-computed:

1) set of $p$ interval thresholds $r_s^k, s=1...p$ for each coordinate $k=1...m$.

2) Matrix of $a$-coefficients defined by (\ref{PQSQ_acoeffs}) based on interval definitions: $a_s^k$, $s=0...p$, $k=1...m$ separately for each coordinate $k$.

Minimization of PQSQ-based functional consists in two steps:

1) For each coordinate $k$, split all data point indices into non-overlapping sets $\mathcal{R}_s^k$:

\begin{equation}
\mathcal{R}_s^k= \{i: r_{s}^k \leq |x_i^k-\beta^k_i| < r_{s+1}^k\}, s = 0...p,
\end{equation}

\noindent where $\bf{\beta}$ is a matrix which depends on the nature of the algorithm.

2) Minimize PQSQ-based functional where each set of points $\{x_{i\in \mathcal{R}_s^k}\}$ contributes
to the functional quadratically with coefficient $a_s^k$. This is a quadratic optimization task.

3) Repeat (1)-(2) till convergence.

\section{General theory of the piece-wise convex potentials as the cone of minorant functions}\label{ConvergenceSection}

In order to deal in most general terms with the data approximation algorithms based on PQSQ potentials, let us consider a general case where a potential can be constructed from a set of functions $\{q_i(x)\}$ with only two requirements: 1) that each $q_i(x)$ has a (local) minimum; 2) that the whole set of all possible $q_i(x)$s forms a cone. In this case, instead of the operational definition (\ref{PQSQ_f}) it is convenient to define the potential $u(x)$ as the minorant function for a set of functions as follows. For convenience, in this section, $x$ will notify a vector $\vec{x}\in R^m$.

Let us consider {\it a generating cone of functions} $Q$. We remind that the definition of a cone implies that for any $q(x)\in Q, p(x)\in Q$, we have $\alpha q(x)+\beta p(x)\in Q$, where $\alpha\geq 0, \beta \geq 0$.

For any finite set of functions ${q_1(x)\in Q, q_2(x)\in Q,..., q_s(x)\in Q}$, we define the minorant function (Figure \ref{OptimizationFigure}):

\begin{equation}\label{minorant}
u_{q_1,q_2,...,q_s}(x) = \min(q_1(x),q_2(x),...,q_s(x)).
\end{equation}

It is convinient to introduce a multiindex $I_{q_1,q_2,...,q_s}(x)$ indicating which particular function(s) $q_i$ corresponds to the value of $u(x)$, i.e.

\begin{equation}\label{minorant_index}
I_{q_1,q_2,...,q_s}(x) = \{i|u_{q_1,q_2,...,q_s}(x)=q_i(x)\}.
\end{equation}

For a cone $Q$ let us define a set of all possible minorant functions $\mathbb{M}(Q)$

\begin{equation}\label{minorant_index}
\begin{split}
\mathbb{M}(Q) = \{ u_{q_{i_1},q_{i_2},...,q_{i_n}} | q_{i_1}\in Q, q_{i_2}\in Q, \\ q_{i_n}\in Q, n = 1,2,3,\dots \}.
\end{split}
\end{equation}

\begin{prop}\label{Misacone}
$\mathbb{M}(Q)$ is a cone.
\end{prop}
\begin{proof}
For any two minorant functions $u_{q_{i_1},q_{i_2},...,q_{i_k}}, u_{q_{j_1},q_{j_2},...,q_{j_s}}\in \mathbb{M}(Q)$ we have

\begin{equation}
\begin{split}
\alpha u_{q_{i_1},q_{i_2},...,q_{i_k}} + \beta u_{q_{j_1},q_{j_2},...,q_{j_s}} = \\ u_{\{\alpha q_{i_p}+\beta q_{j_r}\}} \in \mathbb{M}(Q), \\ p=1,\dots,k, r=1,\dots,s,
\end{split}
\end{equation}

\noindent where ${\{\alpha q_{i_p}+\beta q_{j_r}\}}$ is a set of all possible linear combinations of functions from $\{q_{i_1},q_{i_2},...,q_{i_k}\}$ and $\{q_{j_1},q_{j_2},...,q_{j_s}\}$.
\end{proof}

\begin{prop}\label{Mrestrictedisacone}
Any restriction of $\mathbb{M}(Q)$ onto a linear manifold $L$ is a cone.
\end{prop}
\begin{proof}
Let us denote $q(x)|_L$ a restriction of $q(x)$ function onto $L$, i.e. $q(x)|_L = \{q(x)|x\in L\}$. $q(x)|_L$ is a part of $Q$.
Set of all $q(x)|_L$ forms a restriction $Q|_L$ of $Q$ onto $L$. $Q|_L$ is a cone, hence, $\mathbb{M}(Q)|_L = \mathbb{M}(Q|_L)$ is a cone (Proposition \ref{Misacone}).
\end{proof}

\begin{definition}{\it Splitting algorithm} minimizing $u_{q_{1},q_{2},...,q_{n}}(x)$ is defined as \textbf{Algorithm \ref{MinorantMinimum}}.
\end{definition}

\begin{algorithm}
\caption{Finding local minimum of a minorant function $u_{q_{1},q_{2},...,q_{n}}(x)$}\label{MinorantMinimum}
\begin{algorithmic}[1]
\Procedure{Minimizing minorant function}{}
\State $\textit{initialize } x \gets x_0$
\BState \emph{repeat utill stopping criterion has been met}:
\State $\textit{compute multiindex } I_{q_1,q_2,...,q_s}(x)$
\State \textbf{for all } $i\in I_{q_1,q_2,...,q_s}(x)$
\State $x_i = \argmin\,q_i(x)$
\State \textbf{end for}
\State $\textit{select optimal } x_i:$
\State $x_{opt} \gets \argmin_{x_i} u(x_i)$
\State $x \gets x_{opt}$
\State $\textit{stopping criteria:}$ check if the multiindex
$I_{q_1,q_2,...,q_s}(x)$ does not change compared
to the previous iteration
\BState \emph{end repeat}:
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{theorem}\label{theoremConvergence} Splitting algorithm (\textbf{Algorithm \ref{MinorantMinimum}}) for minimizing $u_{q_{1},q_{2},...,q_{n}}(x)$ converges in a finite number of steps.
\end{theorem}
\begin{proof}
Since the set of functions $\{q_{1},q_{2},...,q_{n}\}$ is finite then we only have to show that at each step the value of the function $u_{q_{1},q_{2},...,q_{n}}(x)$ can not increase. For any $x$ and the value $x' = \argmin q_i(x)$ for $i\in I_{q_1,q_2,...,q_s}(x)$ we can have only two cases:

(1) Either $I_{q_1,q_2,...,q_s}(x)=I_{q_1,q_2,...,q_s}(x')$ (convergence, and in this case $q_{i'}(x')=q_i(x)$ for any $i'\in I_{q_1,q_2,...,q_s}(x')$);

(2) Or $u_{q_{1},q_{2},...,q_{n}}(x)<u_{q_{1},q_{2},...,q_{n}}(x')$ since, accordingly to the definition (\ref{minorant}), $q_{i'}(x')<q_i(x)$, for any $i'\in I_{q_1,q_2,...,q_s}(x'), i\in I_{q_1,q_2,...,q_s}(x)$ (see Figure \ref{OptimizationFigure}).
\end{proof}

Note that in \textbf{Algorithm \ref{MinorantMinimum}} we do not specify exactly the way to find the local minimum of $q_i(x)$. To be practical, the cone $Q$ should contain only functions for which finding a local minimum is fast and explicit. Evident candidates for this role are positively defined quadratic functionals $q(x)=q_{0}+(\vec{q_1},x)+(x,\mathbb{Q}_2x)$, where $\mathbb{Q}_2$ is a positively defined symmetric matrix. Any minorant function (\ref{minorant}) constructed from positively defined quadratic functions will automatically provide subquadratic growth, since the minorant can not grow faster than any of the quadratic forms by which it is defined.

Operational definition of PQSQ given above (\ref{PQSQ_f}), corresponds to a particular form of the quadratic functional, with $\mathbb{Q}_2$ being diagonal matrix. This choice corresponds to coordinate-wise definition of data approximation error function (\ref{distance_function}) that are particularly simple to minimize. This circumstance is used in \textbf{Algorithms \ref{PQSQ_Mean_Algorithm},\ref{PQSQ_PC1}}.

\begin{figure}[h]
\centering\includegraphics[width=0.9\linewidth]{OptimizationAlgorithm.eps}
\caption{Optimization of a one-dimensional minorant function $u(x)$, defined by three functions $q_1(x),q_2(x),q_3(x)$ each of which has a local     minimum.
Each optimization step consists in determining which $q_{I(x)}(x)=u(x)$ and making a step into the local minimum of $q_{I(x)}$. \label{OptimizationFigure}}
\end{figure}


\section{Commonly used data approximators with PQSQ potential}

\subsection{Mean value and $k$-means clustering in PQSQ approximation measure}

Mean vector $\bar{X}_L$  for a set of vectors $X=\{x_i^k\}$, $i=1...N, k=1...m$ and an approximation error defined by potential $u(x)$ can be defined as a point minimizing the mean error potential for all points in  $X$:

\begin{equation}\label{meanDef}
\sum_i\sum_k u(x_i^k-\bar{X}^k) \rightarrow \min.
\end{equation}

For Euclidean metrics $L_2$ ($u(x)=x^2$) it is the usual arithmetric mean.

For $L_1$ metrics ($u(x)=|x|$), (\ref{meanDef}) leads to the implicit equation $\#(x_i^k>\bar{X}^k)=\#(x_i^k<\bar{X}^k)$, where $\#$ stands for the number of points, which corresponds to the definition of median. This equation can not have a unique solution in case of even number of points or when some data point coordinates coincide: therefore, definition of median is usually accompanied by heuristics used for breaking ties, i.e. to deal with non-uniquely defined rankings. This situation reflects the general situation of existence of multiple local minimuma and possible non-uniqueness of global minimum of (\ref{meanDef}) (Figure~\ref{MeanValueFigure}).

For PQSQ approximation measure (\ref{PQSQ_f}) it is difficult to write down an explicit formula for computing the mean value
corresponding to the global minimum of (\ref{meanDef}).
In order to find a point $\bar{X}_{PQSQ}$ minimizing mean $PQSQ$ potential, a simple iterative algorithm can be used:

%%\begin{algorithm}
%%\caption{Computing PQSQ mean value}\label{PQSQ_mean}
%%\begin{algorithmic}[1]
%%\Procedure{PQSQ Mean}{}
%%\State $\textit{stringlen} \gets \text{length of }\textit{string}$
%%\State $i \gets \textit{patlen}$
%%\BState \emph{top}:
%%\If {$i > \textit{stringlen}$} \Return false
%%\EndIf
%%\State $j \gets \textit{patlen}$
%%\BState \emph{loop}:
%%\If {$\textit{string}(i) = \textit{path}(j)$}
%%\State $j \gets j-1$.
%%\State $i \gets i-1$.
%%\State \textbf{goto} \emph{loop}.
%%\State \textbf{close};
%%\EndIf
%%\State $i \gets i+\max(\textit{delta}_1(\textit{string}(i)),\textit{delta}_2(j))$.
%%\State \textbf{goto} \emph{top}.
%%\EndProcedure
%%\end{algorithmic}
%%\end{algorithm}

\begin{algorithm}
\caption{Computing PQSQ mean value}\label{PQSQ_Mean_Algorithm}
\begin{algorithmic}[1]
\Procedure{PQSQ Mean Value}{}
\State $\textit{define intervals } r_s^k, s=0...p, k=1...m$
\State $\textit{compute coefficients } a_s^k$
\State $\textit{initialize } \bar{X}_{PQSQ}  \linebreak \textit{eg., by arithmetic mean}$
\BState \emph{repeat till convergence of $\bar{X}_{PQSQ}$}:
\State \textbf{for each } \textit{coordinate } $k$
\State \textit{define sets of indices}
\begin{equation}
\begin{split}
\mathcal{R}_s^k=\{i: r_{s}^k \leq |x_i^k-\bar{X}_{PQSQ}^k| < r_{s+1}^k\}, \\ s = 0,\dots,p \nonumber
\end{split}
\end{equation}
\State \textit{compute new approximation for } $\bar{X}_{PQSQ}$:
\State $\bar{X}_{PQSQ}^k \gets \frac{\sum_{s=1...p}a_s^k\sum_{i\in \mathcal{R}_s^k}x_i^k}{\sum_{s=1...p}a_s^k|\mathcal{R}_s^k|}$
\State \textbf{end for}
\State \textbf{goto} \emph{repeat till convergence}
\EndProcedure
\end{algorithmic}
\end{algorithm}

The suggested algorithm converges to the local minimum which depends on the initial point approximation.

Based on the PQSQ approximation measure and the algorithm for computing the PQSQ mean value (\ref{PQSQ_Mean_Algorithm}), one can construct the PQSQ-based $k$-means clustering procedure in the usual way, splitting estimation of cluster centroids given partitioning of the data points into $k$ disjoint groups, and then re-calculating the partitioning using the PQSQ-based proximity measure.

\begin{figure}[h]
\centering\includegraphics[width=1\linewidth]{MeanValue.eps}
\caption{Minimizing the error to a point (finding the mean value) for a set of 4 points (shown by black circles). Solid red line corresponds to L1-based error. Thing blue lines correspond to PQSQ error potential imitating the L1-based error. Several choices of PQSQ potential for different numbers of intervals (indicated by a number put on top of the line) is illustrated. On the right panel a zoom of a particular region of the left plot is shown. Neither function (L1-based or PQSQ-based) possesses a unique local minimum. Moreover, L1-based error function has infinite number of points corresponding to the global minimum (any number between 3 and 4), while PQSQ error function has several local minimuma in [3;4] interval which exact positions are sensitive to the concrete choice of PQSQ parameters (interval definitions).}\label{MeanValueFigure}
\end{figure}


\subsection{Principal Component Analysis (PCA) in PQSQ metrics}

Accordingly to the classical definition of the first principal component, it is a line best fit to the data set $X$ \cite{Pearson1901On}. Let us define a line in the parametric form $\vec{y}=\vec{V}u+\vec{\delta}$, where $u \in R^1$ is the parameter. Then the first principal component will be defined by vectors $\vec{V}, \vec{\delta}$ satisfying

\begin{equation}
\sum_i\sum_k u(x_i^k-V^ku_i-\delta^k) \rightarrow \min,
\end{equation}

\noindent where

\begin{equation}
u_i = \arg \min_s \sum_k u(x_i^k-V^ks-\delta^k).
\end{equation}

The standard first principal component (PC1) corresponds to $u(x)=x^2$ when the vectors $\vec{V}, \vec{\delta}$ can be found by a simple iterative splitting algorithm for Singular Value Decomposition (SVD). If $X$ does not contain missing values then $\vec{\delta}$ is the vector of arithmetic mean values. By contrast, computing $L1$-based principal components ($u(x)=|x|$) represents a much more challenging optimization problem \cite{Brooks2013}. Several approximative algorithms for computing $L1$-norm PCA have been recently suggested and benchmarked \cite{Ke2005,Kwak2008,Brooks2013,brooks2012pcal1,Park2014}. To our knowledge, there have not been a general efficient algorithm suggested for computing PCA in case of arbitrary approximation measure for some monotonous function $u(x)$.

Computing PCA based on PQSQ approximation error is only slightly more complicated than computing the standard $L2$ PCA by SVD. Here we provide a pseudo-code (\textbf{Algorithm \ref{PQSQ_PC1}}) of a simple iterative algorithm (similar to \textbf{Algorithm \ref{PQSQ_Mean_Algorithm}}) with guaranteed convergence (see Section \ref{ConvergenceSection}).

\begin{algorithm}
\caption{Computing PQSQ PCA}\label{PQSQ_PC1}
\begin{algorithmic}[1]
\Procedure{PQSQ First Principal Component}{}
\State $\textit{define intervals } r_s^k, s=0...p, k=1...m$
\State $\textit{compute coefficients } a_s^k$
\State $\vec{\delta} \gets \bar{X}_{PQSQ}$
\State $\textit{initialize } \vec{V} \textit{ : eg., by L2-based PC1}$
\State $\textit{initialize } \{u_i\} \textit{ : eg., by } \linebreak $$u_i = \frac{\sum_k V^k(x_i^k-\delta^k)}{\sum_k (V^k)^2}$$ $
\BState \emph{repeat till convergence of $\vec{V}$}:
\State $\textit{normalize } \vec{V} \textit{ : } \vec{V} \gets \frac{\vec{V}}{||\vec{V}||}$
\State \textbf{for each } \textit{coordinate } $k$
\State \textit{define sets of indices}
\begin{equation}
\begin{split}
\mathcal{R}_s^k=\{i: r_{s}^k \leq |x_i^k-V^ku_i-\delta^k| < r_{s+1}^k\}, \\ s = 0...p \nonumber
\end{split}
\end{equation}
\State \textbf{end for}
\State \textbf{for each } \textit{data point } $i$ and \textit{coordinate } $k$
%\State \textit{compute new approximation for } $\{u_i\}$:
\State \textit{find all $s_{i,k}$ such that $i\in \mathcal{R}_{s_{i,k}}^k$}
\State \textbf{if}\textit{ all $a^k_{s_{i,k}}=0$ \textbf{then} $u'_i \gets 0$} \textbf{else}
\State
$$u'_i \gets \frac{\sum_{k}a_{s_{i,k}}^kV^k(x_i^k-\delta^k)}{\sum_{k}a_{s_{i,k}}^k(V^k)^2}$$
\State \textbf{end for}
\State \textbf{for each } \textit{coordinate } $k$
%\State \textit{compute new $\vec{V}, \vec{\delta}$}:
%\State \textit{solve $2\times 2$ system of equations for $V^k$, $\delta^k$}:
$$
%\begin{cases}
%V^k \times \sum_s a_s^k \sum_{i\in \mathcal{R}_s^k}u_i + \delta^k \times \sum_s a_s^k |\mathcal{R}_s^k| = \sum_s a_s^k \sum_{i\in \mathcal{R}_s^k}x_i^k \\
%V^k \times \sum_s a_s^k \sum_{i\in \mathcal{R}_s^k}(u_i)^2 + \delta^k \times \sum_s a_s^k \sum_{i\in \mathcal{R}_s^k}u_i = \sum_s a_s^k \sum_{i\in \mathcal{R}_s^k}x_i^ku_i
V^k \gets \frac{\sum_s a_s^k \sum_{i\in \mathcal{R}_s^k}(x_i^k-\delta^k)u_i}{\sum_s a_s^k \sum_{i\in \mathcal{R}_s^k}(u_i)^2}
%\end{cases}
$$
\State \textbf{end for}
\State \textbf{for each } $i$ :
\State $u_i \gets u'_i$
\State \textbf{end for}
\State \textbf{goto} \emph{repeat till convergence}
\EndProcedure
\end{algorithmic}
\end{algorithm}

Computation of second and further principal components follows the standard deflation approach: projections of data points onto the previously computed component are subtracted from the data set, and the algorithm is applied to the residues. However, as it is the case in any non-quadratic metrics, the resulting components can be non-orthogonal or even not invariant with respect to the dataset rotation. Moreover, unlike $L2$-based principal components, the \textbf{Algorithm \ref{PQSQ_PC1}} do not always converge to a unique global minimum; the computed components can depend on the initial estimate of $\vec{V}$. The situation is somewhat similar to the standard $k$-means algorithm. Therefore, in order to achieve the least possible approximation error to the linear subspace, $\vec{V}$ can be initialized randomly or by data vectors $\vec{x}_i$ many times and the deepest in PQSQ approximation error (\ref{distance_function}) minimum should be selected.

How does the \textbf{Algorithm \ref{MinorantMinimum}} serve a more abstract version of the \textbf{Algorithms \ref{PQSQ_Mean_Algorithm},\ref{PQSQ_PC1}}? For example, the ``variance" function $m(\vec{x})=\frac{1}{N}\sum_j u(\vec{x}_j-\vec{x})$ to be minimized in \textbf{Algorithm \ref{PQSQ_Mean_Algorithm}} uses the generating functions in the form $Q = \{b_{ji}^k+\sum_k a_{ji}^k(x^k-x_j^k)^2\}$, where $i$ is the index of the interval in (\ref{PQSQ_f}). Hence, $m(x)$ is a minorant function, belonging to the cone $\mathbb{M}(Q)$, and must converge (to a local minimum) in a finite number of steps accordingly to Theorem \ref{theoremConvergence}.


\subsection{Nonlinear methods: PQSQ-based Principal Graphs and Manifolds}

In a series of works, the authors of this article introduced a family of methods
for constructing principal objects
based on graph approximations (e.g., principal curves, principal manifolds, principal trees),
which allows constructing explicit non-linear data approximators
(and, more generally, approximators with non-trivial topologies, suitable for approximating,
e.g., datasets with branching or circular topology) \cite{Gorban1999, Gorban2001ihespreprint, gorban2001method, gorban2005elastic, gorban2007topological,Gorban2008Principal,Gorban2009,Gorban2010}. The methodology is
based on optimizing a piece-wise quadratic {\it elastic energy} functional (see short description below).
A convenient graphical user interface was developed with
implementation of some of these methods \cite{Gorban2014}.

Let $G$ be a simple undirected graph with set of vertices $Y$ and
set of edges $E$. For $k \geq 2$ a $k$-star in $G$ is a subgraph
with $k+1$ vertices $y_{0,1, \ldots k} \in Y$ and $k$ edges $\{(y_0,
y_i) \ | \ i=1,\ldots k\} \subset E$. Suppose for each $k\geq 2$, a
family $S_k$ of $k$-stars in $G$ has been selected. We call a graph
$G$ with selected families of $k$-stars $S_k$ an {\it elastic graph}
if, for all $E^{(i)} \in E $ and $S^{(j)}_k \in S_k$, the
correspondent elasticity moduli $\lambda_i > 0$ and $\mu_{kj}
> 0$ are defined. Let  $E^{(i)}(0),E^{(i)}(1)$ be vertices of an
edge $E^{(i)}$ and $S^{(j)}_k (0),\ldots S^{(j)}_k (k)$ be vertices
of a $k$-star  $S^{(j)}_k $ (among them, $S^{(j)}_k (0)$ is the
central vertex).

For any map $\phi:Y \to R^m$ the {\it energy of the
graph} is defined as


\begin{equation}\label{elastic_energy}
\begin{split}
U^{\phi}{(G)}:=  \sum_{E^{(i)}} \lambda_i
\left\|\phi(E^{(i)}(0))-\phi(E^{(i)}(1)) \right\| ^2 + \\
+ \sum_{S^{(j)}_k}\mu_{kj} \left\|\sum _ {i=1}^k \phi(S^{(j)}_k
(i))-k\phi(S^{(j)}_k (0)) \right\|^2. \nonumber
\end{split}
\end{equation}

For a given map $\phi: Y \to R^m$ we divide the dataset $D$ into
node neighborhoods $K^y, \, y\in Y$. The set $K^y$ contains the data points for
which the node $\phi(y)$ is the closest one in $\phi(y)$. The {\it
energy of approximation} is:

\begin{equation}\label{approximation_term}
U^{\phi}_A(G,D)= \sum_{y \in Y} \sum_{ x \in K^y} w(x) \|x-
\phi(y)\|^2,
\end{equation}
where $w(x) \geq 0$ are the point weights. Simple and fast algorithm for minimization of the energy

\begin{equation}\label{globalStandardEnergy}
U^{\phi}=U^{\phi}_A(G,D)+U^{\phi}{(G)}
\end{equation}

\noindent is the splitting algorithm, in the spirit of the classical $k$-means clustering: for a given
system of sets $\{K^y \ | \ y \in Y \}$ we minimize $U^{\phi}$ (optimization step, it
is the minimization of a positive quadratic functional), then for a
given $\phi$ we find new $\{K^y\}$ (re-partitioning), and so on; stop when no change.

Application of PQSQ-based potential is straightforward in this approach. It consists
in replacing (\ref{approximation_term}) with

\begin{equation}\label{approximation_term_PQSQ}
U^{\phi}_A(G,D)= \sum_{y \in Y} \sum_{ x \in K^y} w(x) \sum_k u(x^k-
\phi(y^k)),\nonumber
\end{equation}

\noindent where $u$ is a chosen PQSQ-based error potential. Partitioning of the dataset
into $\{K^y\}$ can be also based on calculating the minimum PQSQ-based error to $y$, or can
continue enjoying nice properties of $L2$-based distance calculation.

\section{Numerical examples}

\subsection{Practical choices of parameters}

The main parameters of PQSQ are (a) majorating function $f(x)$ and (b) decomposition of each coordinate range into $p+1$ non-overlapping intervals.
Depending on these parameters, various approximation error properties can be exploited, including robustness to outlier data points.

When defining the intervals $r_j, j=1\dots p$, it is desirable to achieve a small difference between $f(\Delta x)$ and $u(\Delta x)$ for expected argument values $\Delta x$ (differences between an estimator and the data point), and choose the suitable value of the potential trimming threshold $r_p$ in order to achieve the desired robustness properties. If no trimming is needed, then $r_p$ should be made larger than the maximum expected difference between coordinate values (maximum $\Delta x$).

In our numerical experiments we used the following definition of intervals. For any data coordinate $k$, we define a characteristic difference $D^k$, for example

\begin{equation}\label{characteristic_distance_amplitude}
D^k = \alpha_{scale}(max_i(x_i^k)-min_i(x_i^k)),
\end{equation}

\noindent where $\alpha_{scale}$ is a scaling parameter, which can be put at 1 (in this case, the approximating potential will not be trimmed). In case of existence of outliers, for defining $D^k$, instead of amplitude one can use other measures such as the median absolute deviation (MAD):

\begin{equation}\label{characteristic_distance_mad}
D^k = \alpha_{scale}median_i(|x_i^k-median(\{x_i^k\}|);
\end{equation}

\noindent in this case, the scaling parameter should be made larger, i.e. $\alpha_{scale}=10$, if no trimming is needed.

After defining $D^k$ we use the following definition of intervals:

\begin{equation}\label{intervals_definition}
r_j^k = D^k\frac{j^2}{p^2}, j=0\dots p.
\end{equation}

More sophisticated approaches are also possible to apply such as, given the number of intervals $p$ and the majorating function $f(x)$, choose $r_j, j=1\dots p$ in order to minimize the integral difference

$$
\int_0^{r_p}(f(x)-u(x))^2dx \rightarrow \min.
$$

In further examples, we use (\ref{characteristic_distance_amplitude}) and (\ref{intervals_definition}) to define intervals in (\ref{PQSQ_f}).

\subsection{Implementation}

We provide Matlab implementation of PQSQ approximators (in particular, PCA) at https://github.com/auranic/PQSQ-DataApproximators.
Preliminary version of the Java code implementing also elastic graph-based non-linear approximator implementations are available from
the authors on request.

%We also provide Java implementation of PQSQ-based approximators (PCA, principal graphs) as a part of \emph{vdaoengine} library at https://github.com/auranic/VDAOEngine. The code is %accompanied by examples of application.
%

\subsection{Simple benchmark test}

In order to compare the computation time and precision of PQSQ-based PCA algorithm for the case $u(x)=|x|$ with existing R-based implementations of $L1$-based PCA methods (pcaL1 package), we follow the benchmark described in \cite{brooks2012pcal1}. We compare performance of PQSQ-based PCA based on \textbf{Algorithm \ref{PQSQ_PC1}} with several $L1$-based PCA algorithms: L1-PCA* \cite{Brooks2013}, L1-PCA \cite{Ke2005}, PCA-PP \cite{Croux2007}, PCA-L1 \cite{Kwak2008}. As a reference point, we use the standard PCA algorithm based on quadratic norm and computed using the standard SVD iterations.

The idea of benchmarking is to generate a series of datasets of the same size ($N=1000$ objects in $m=10$ dimensions) such that the first 5 dimensions would be sampled from a uniform distribution $U(-10,10)$. Therefore, the first 5 dimensions represent ``true manifold" sampled by points.

The values in the last 5 dimensions represent ``noise+outlier" signal. The background noise is represented by Laplacian distribution of zero mean and 0.1 variance. The outlier signal is characterized by mean value $\mu$, dimension $p$ and frequency $\phi$. Then, for each data point with a probability $\phi$, in the first $p$ outlier dimensions a value is drawn from $Laplace(\mu,0.1)$. The rest of the values is drawn from background noise distribution.

As in \cite{brooks2012pcal1}, we've generated 1300 test sets corresponding to $\phi=0.1$, with 100 examples for each combination of $\mu=1,5,10,25$ and $p=1,2,3$.

For each test set 5 first principal components $\vec{V}_1,\dots,\vec{V}_{5}$ of unit length were computed, with corresponding point projection distributions $U^1,\dots,U^{5}$ and the mean vector $\vec{C}$. Therefore, for each initial data point $\vec{x}_i$, we have the ``restored" data point $$P(\vec{x}_i)=\sum_{k=1\dots 5}U^k_i\vec{V}_k+\vec{C}.$$

For computing the PQSQ-based PCA we used 5 intervals without trimming. Changing the number of intervals did not significantly varied the benchmarking results.

Two characteristics were measured: (1) computation time measured as a ratio to the computation of 5 first principal components using the standard $L2$-based PCA and (2) the sum of absolute values of the restored point coordinates in the ``outlier" dimensions normalized on the number of points:

\begin{figure}[h]
\centering\includegraphics[width=0.8\linewidth]{benchmark.eps}
\caption{Simple benchmarking of several algorithms for constructing $L1$-based PCA. The computational cost of application of linear programming methods instead of simpler iterative methods is approximately shown by an arrow. \label{benchmark}}
\end{figure}


\begin{equation}
\epsilon = \frac{1}{N}\sum_{i=1\dots N} \sum_{k=6\dots 10} |P^k(\vec{x}_i)|.
\end{equation}

Formally speaking, $\epsilon$ is $L1$-based from a point projection into the 5 principal components to the ``true" subspace. In simple terms, larger values of $\epsilon$ correspond to the situation when the first 5 principal components do not represent well the first ``true" dimensions, having significant loadings into the ``outlier dimensions". Only if the first 5 components do not have any non-zero loadings in the dimensions $6\dots 10$ then $\epsilon=0$.

The results of the comparison, averaged over all 1300 test sets, are shown in Figure~\ref{benchmark}. As one can see, PQSQ-based computation of PCA outperforms by accuracy the existing heuristics such as PCA-L1 but remains computationally efficient being 100 times faster than L1-PCA giving almost the same accuracy and almost 500 times faster than the L1-PCA* algorithm which is, however, significantly gains in accuracy. From Figure~\ref{benchmark}, one can see that PQSQ-based approach performs the best in terms of the accuracy
in the family of fast iterative methods.

The detailed tables of comparison for all combinations of parameters are provided in Supplementary Table 1. The scripts used to generate the datasets and compare the results can be found at https://github.com/auranic/PQSQ-DataApproximators.

\section{Conclusion}

In this paper we propose a method of constructing the standard data approximators (mean value, $k$-means clustering, principal components, principal graphs)
for arbitrary non-quadratic approximation error with subquadratic growth by using a piecewise-quadratic error functional (PQSQ potential). These approximators can be computed
by applying quasi-quadratic optimization procedures, which are simple adaptations of the previously described standard and computationally efficient algorithms.

\begin{table*}
\begin{tabular*}{\textwidth}{p{6cm}p{7cm}}
\hline
\multicolumn{2}{c}{Data approximation/Clustering/Manifold learning} \\
\hline
Principal Component Analysis &  Includes robust trimmed version of PCA, $L1$-based PCA, regularized PCA, and many other PCA modifications \\
Principal curves and manifolds &  Provides possibility to use non-quadratic data approximation terms and trimmed robust version\\
Self-Organizing maps &  Same as above \\
Principal graphs/trees &  Same as above \\
K-means & Can include adaptive error potentials based on estimated error distributions inside clusters \\
\hline
\multicolumn{2}{c}{High-dimensional data mining} \\
\hline
Use of fractional metrics & Introducing fractional metrics in existing data-mining techniques can potentially deal with the curse of dimensionality, helping to better distinguish close from distant data points \cite{Aggarwal2001} \\
\hline
\multicolumn{2}{c}{Regularized regression} \\
\hline
Lasso & Application of PQSQ-based potentials should lead to speeding up the algorithm in case of large datasets \\
Elastic net & Same as above \\
\hline
\end{tabular*}
\caption{List of methods which can use PQSQ-based error potentials}\label{ApplicationTable}
\end{table*}


The suggested methodology have several advantages over existing ones:

(a) \textit{Scalability}: the algorithms are computationally efficient and can be applied to large data sets containing millions of numerical values.

(b) \textit{Flexibility}: the algorithms can be adapted to any type of data metrics with subquadratic growth, even if the metrics can not be expressed in explicit form. For example, the error potential can be chosen as adaptive metrics \cite{Yang2006, Wu2009}.

(c) \textit{Built-in (trimmed) robustness}: choice of intervals in PQSQ can be done in the way to achieve a trimmed version of the standard data approximators, when points distant from the approximator do not affect to the error minimization during the current optimization step.

(d) \textit{Guaranteed convergence}: the suggested algorithms converge to local or global minimum just as the corresponding predecessor algorithms based on quadratic optimization and expectation/minimization-based splitting approach.

One of the application of the suggested methodology is approximating the popular in data mining $L1$ metrics. We show by numerical simulations that PQSQ-based approximators perform as fast as the fast heuristical algorithms for computing $L1$-based PCA but achieve better accuracy in a previously suggested benchmark test. PQSQ-based approximators are less accurate than the exact algorithms for optimizing $L1$-based functions utilizing linear programming: however, they are several orders of magnitude faster.

At the same time, PQSQ-based approximators can imitate a variety of subquadratic error potentials, including fractional ones. In Table~\ref{ApplicationTable} we give a possible range of applications of PQSQ-based approximators. Evidently, PSQS potential can be applied in the task of regression, replacing the classical Least Squares or L1-based Least Absolute Deviation methods. Moreover, PQSQ potential can be easily adapted to the problems of regularized regression with non-quadratic penalty onto the regression coefficients, such as LASSO \cite{Tibshirani1996} or Elastic Net \cite{Zou2005}.



\bibliographystyle{model1-num-names}
\bibliography{SubquadraticPotential}

%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use model1-num-names.bst.

%% References without bibTeX database:

% \begin{thebibliography}{00}

%% \bibitem must have the following form:
%%   \bibitem{key}...
%%

% \bibitem{}

% \end{thebibliography}


\end{document}

%%
%% End of file `elsarticle-template-1-num.tex'.
